{"pages":[{"title":"关于","text":"一个在努力的奔跑的90后 Wang Yang mail: wangyang7902@gmail.com Software Engineer Linux Driver Android驱动 GPU Driver","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"DRM GPU Scheduler","text":"DRM GPU Scheduler 背景调度器 顾名思义是用来分配某种有限资源的一种算法。 从CPU的视角来看，CPU的核心数目是远少于线程数目的，所以在某一个时刻只有一部分线程才可以得到执行，控制这些这些线程执行的时机和时长的一种算法，我们称之为CPU的调度算法。同样的在GPU领域也是同理的，在现代的操作系统中，每一个进程都期望可以使用GPU资源进行某种计算，所以GPU的调度器也就有所必要，但是与CPU相比GPU的调度器还是相对来说比较简单的。 GPU的硬件资源 (AMD) VMID (Virtual Machine ID) 每一个VMID对应一组GPU的页表的寄存器 GFX Engine GPU 3D Graphics 图像处理单元 Compute Engine GPU Compute 处理单元 sDMA (system Direct Memory Access) GPU内部的DMA单元 VCN (Video Codec Next) Engine 视频编解码单元 JPEG 图像处理单元 …. 硬件资源的个数会根据GPU的类型有所不同，例如在GPU计算卡领域并不需要图形计算，因此并不需要GFX相关的硬件单元，并且在计算领域可能需要更多的数据拷贝，所以因此会包含多个sDMA单元，这种设计可以很好的是陪产品的不同定位。 在GPU Driver中，驱动会为每一种硬件单元创建一个或者多个Ring，这些Ring本质是一个环形的缓冲区，充当软件和硬件沟通的桥梁，软件（用户）作为生产者负责提交命令到Ring中，硬件单元负责从这个Ring中获得要执行的命令，这样软件和硬件就建立某种映射关系。因此每一个线程（包括内核线程）都可以提交命令到这个Ring中，因此GPU的调度器存在就很有必要了。 DRM GPU Scheduler 的调度算法GPU里的每一个硬件单元提供一个或者多个Ring来与软件进行交互，GPU scheduler会为每一个Ring创建不同等级的RQ队列，由调度线程完成调度。 12345678enum drm_sched_priority { DRM_SCHED_PRIORITY_MIN, DRM_SCHED_PRIORITY_NORMAL, DRM_SCHED_PRIORITY_HIGH, DRM_SCHED_PRIORITY_KERNEL, DRM_SCHED_PRIORITY_COUNT, DRM_SCHED_PRIORITY_UNSET = -2 }; 主要调度策略如下： Priority 如果高等级的RQ里有job，低等级的RQ就不能运行 RR (Round-Robin) 在相同的RQ里按照RR算法，分别运行没一个entity里的job，直至当RQ的job都被调度，在调度下一优先级 RR的算法可以避免进程饿死的问题，但是又没有很好的解决调度延迟的问题，是当前调度算法的一种妥协 数字仅仅是本文章用来标识job，调度器内部并没有使用这些数字。 以GFX的调度为例，GFX 调度器其由4个优先级的RQ组成，每一个RQ里有多个entity，每一个entity由多个job组成的一个队列。 调度算法会先在高优先级RQ里选择一个需要调度的entity，在从选中的entity里挑选出一个job进行运行，直至当前优先级里的所有job运行完毕，在从下一个优先级继续运行，直至所有job运行完成，没有job就绪的时候，调度线程会睡眠，当有新的job 就绪的时候，调度线程会唤醒。 按照现有的调度策略，job按照如下顺序运行： 101 -&gt; 201 -&gt; 102 -&gt; 202 -&gt; 103 -&gt; 203 -&gt; 104 301 -&gt; 302 -&gt; 303 401 -&gt; 501 -&gt; 601 -&gt; 402 -&gt; …. -&gt; 603 701 -&gt; 702 DRM GPU Scheduler 的负载均衡GPU Scheduler和CPU一样提供了负载均衡的功能，可以平衡不同硬件单元的负载，以提高硬件整体的利用率。 以 AMD Radeon RX 5500 为例，硬件层面提供了2个sDMA单元，每一个硬件单元的功能是完全相同的，因此可以将负载比较高的RQ上的job移动到负载比较低的RQ上进行调度，从而提高硬件整体的利用率和吞吐率。 负载均衡的时机CPU线程最好的负载均衡时机是线程刚创建的时候，因为这个时候所有TLB都是冷的，如果线程运行一段时间TLB变热之后，负载均衡的成本是相对较高的。 同CPU一样，GPU的job也是在刚创建的时候进行负载均衡成本是最低的，所有TLB对于这个新出生的job是一样的，不过与CPU不同的是，GPU的job通常来说只会运行一次，下次运行需要重新提交新的job，因此负载均衡的时机只能发生在job创建的阶段。 负载均衡的算法当前GPU调度器没有办法推测job运行的时间长短，也无法衡量job对硬件资源的负载贡献，因此现阶段简单暴力的用job的数量来作为负载均衡的唯一条件。 这是相当不严谨的，job仅仅是软件层面对GPU命令的一种抽象，每一个job对GPU的负载是不一样的，但是好在一个job只会执行一次。 关键代码分析： 123456789101112131415161718192021222324struct drm_gpu_scheduler *drm_sched_pick_best(struct drm_gpu_scheduler **sched_list, unsigned int num_sched_list){ struct drm_gpu_scheduler *sched, *picked_sched = NULL; int i; unsigned int min_score = UINT_MAX, num_score; for (i = 0; i &lt; num_sched_list; ++i) { sched = sched_list[i]; if (!sched-&gt;ready) { DRM_WARN(&quot;scheduler %s is not ready, skipping&quot;, sched-&gt;name); continue; } num_score = atomic_read(&amp;sched-&gt;score); if (num_score &lt; min_score) { min_score = num_score; picked_sched = sched; } } return picked_sched;} 调度算法先通过函数drm_sched_pick_best在同类型的调度器里选择运行job数量最少的硬件单元。 1234567891011121314151617181920212223void drm_sched_entity_select_rq(struct drm_sched_entity *entity){ struct dma_fence *fence; struct drm_gpu_scheduler *sched; struct drm_sched_rq *rq; if (spsc_queue_count(&amp;entity-&gt;job_queue) || entity-&gt;num_sched_list &lt;= 1) return; fence = READ_ONCE(entity-&gt;last_scheduled); if (fence &amp;&amp; !dma_fence_is_signaled(fence)) return; spin_lock(&amp;entity-&gt;rq_lock); sched = drm_sched_pick_best(entity-&gt;sched_list, entity-&gt;num_sched_list); rq = sched ? &amp;sched-&gt;sched_rq[entity-&gt;priority] : NULL; if (rq != entity-&gt;rq) { drm_sched_rq_remove_entity(entity-&gt;rq, entity); entity-&gt;rq = rq; } spin_unlock(&amp;entity-&gt;rq_lock);} 把当的entity迁移到与同等优先级的RQ上来完成负载均衡，值得注意的是，负载均衡的基本单位是entity不是job。 DRM GPU Scheduler 的事件 事件 Scheduler Fence 类型 说明 job已经schedule scheduled 标识job已经投入运行 job执行完成 finished 标识job已经运行完成 scheduler使用dma_fence来与外部接口完成事件的同步，在内核中dma_fence常作为一种异步同步机制存在驱动代码中，dma_fence主要提供2种功能： Wait: 等待某一事件是否触发，如果没事件没有触发，会让当前线程睡眠 Signal: 标识某一个事件处罚，dma_fence触发后会一次调用绑定在当前fence上的所有callback函数，来分发某一事件，来完成事件同步的机制。 DRM GPU Scheduler 对硬件行为的抽象1234567891011121314151617181920212223242526272829struct drm_sched_backend_ops { /** * @dependency: Called when the scheduler is considering scheduling * this job next, to get another struct dma_fence for this job to * block on. Once it returns NULL, run_job() may be called. */ struct dma_fence *(*dependency)(struct drm_sched_job *sched_job, struct drm_sched_entity *s_entity); /** * @run_job: Called to execute the job once all of the dependencies * have been resolved. This may be called multiple times, if * timedout_job() has happened and drm_sched_job_recovery() * decides to try it again. */ struct dma_fence *(*run_job)(struct drm_sched_job *sched_job); /** * @timedout_job: Called when a job has taken too long to execute, * to trigger GPU recovery. */ void (*timedout_job)(struct drm_sched_job *sched_job); /** * @free_job: Called once the job's finished fence has been signaled * and it's time to clean it up. */ void (*free_job)(struct drm_sched_job *sched_job);}; dependency: 用于判断job是否可以投入运行 run_job: 用于提交一个job timeout_job: 当finished fence超过一定的时间没有释放，调度器会通过这个接口通知驱动 free_job: 负责job的清理工作 DRM GPU Scheduler 调度线程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566static int drm_sched_main(void *param){ struct drm_gpu_scheduler *sched = (struct drm_gpu_scheduler *)param; int r; sched_set_fifo_low(current); while (!kthread_should_stop()) { struct drm_sched_entity *entity = NULL; struct drm_sched_fence *s_fence; struct drm_sched_job *sched_job; struct dma_fence *fence; struct drm_sched_job *cleanup_job = NULL; wait_event_interruptible(sched-&gt;wake_up_worker, (cleanup_job = drm_sched_get_cleanup_job(sched)) || (!drm_sched_blocked(sched) &amp;&amp; (entity = drm_sched_select_entity(sched))) || kthread_should_stop()); if (cleanup_job) { sched-&gt;ops-&gt;free_job(cleanup_job); /* queue timeout for next job */ drm_sched_start_timeout(sched); } if (!entity) continue; sched_job = drm_sched_entity_pop_job(entity); complete(&amp;entity-&gt;entity_idle); if (!sched_job) continue; s_fence = sched_job-&gt;s_fence; atomic_inc(&amp;sched-&gt;hw_rq_count); drm_sched_job_begin(sched_job); trace_drm_run_job(sched_job, entity); fence = sched-&gt;ops-&gt;run_job(sched_job); drm_sched_fence_scheduled(s_fence); if (!IS_ERR_OR_NULL(fence)) { s_fence-&gt;parent = dma_fence_get(fence); r = dma_fence_add_callback(fence, &amp;sched_job-&gt;cb, drm_sched_process_job); if (r == -ENOENT) drm_sched_process_job(fence, &amp;sched_job-&gt;cb); else if (r) DRM_ERROR(&quot;fence add callback failed (%d)\\n&quot;, r); dma_fence_put(fence); } else { if (IS_ERR(fence)) dma_fence_set_error(&amp;s_fence-&gt;finished, PTR_ERR(fence)); drm_sched_process_job(NULL, &amp;sched_job-&gt;cb); } wake_up(&amp;sched-&gt;job_scheduled); } return 0;} 通过以上简单的介绍，现在来看DRM GPU的调度算法是比较容易的，简单来分析下 sched_set_fifo_low(current) 会将调度线程设置RT-FIFO调度算法，这样可以以一个相对相对较高的优先级运行，从而减少调度的延迟 entity = drm_sched_select_entity(sched) 选择需要调度entity，每一个entity用来标示一组job, 一个进程可以 创建一个或者多个entity sched_job = drm_sched_entity_pop_job(entity); 每一个entity内部有一个spsc无锁队列，每次调度其中一个job atomic_inc(&amp;sched-&gt;hw_rq_count); hw_rq_count 用来表示有多少job可以同时在硬件中运行，这个在AMD平台设置成2,避免过多的job处于“fly”状态，同时也可以保证硬件吞吐率处于一个比较合理的范围，大致就是ping - pong - ping - pong的节奏，这个功能也会影响scheduler的recover功能 drm_sched_job_begin(sched_job);登记当前job，并且启动一个定时器，来判断这个job后续是否运行超时 fence = sched-&gt;ops-&gt;run_job(sched_job); 提交这个job到硬件，并返回一个硬件fence， 这个fence可以表示job是否在硬件上执行完成 drm_sched_fence_scheduled(s_fence);释放scheduled fence，用来通知外部接口，来表示当前job已经投入运行了。 r = dma_fence_add_callback(fence, &amp;sched_job-&gt;cb, drm_sched_process_job); 将硬件返回的fence和 调度器内部的finished fence进行绑定，硬件fence触发，会导致 scheduler finished fence 触发，finished fence 作为标准接口可以继续通知其他关心这个事件的人。","link":"/2021/01/20/gpuscheduler/"},{"title":"CPU视角下的GPU物理内存 (VRAM&#x2F;GTT)","text":"本文所讨论的均为AMD公司的显卡产品 显卡：AMD Radeon RX 5500 - 8GB 显存 Linux：Linux-5.4.y (fc944ddc0b4a) GPU的分类GPU又称之为显卡，在计算机里面扮演着图形渲染和并行计算的角色，今天我们就来聊一聊GPU的物理内存。 现代的计算机显卡按照内存的类型分为2种： UMA:Unified Memory Architecture, 统一内存架构，一般认为是集成显卡 dGPU: Discrete GPU 独立显卡 独立显卡一般配有高带宽(HBM)的显存，主要有2种：GDDRX 和 HBM显存, HBM主要用于高端显卡。 显存的大小 UMA类型的GPU没有独立的显存，一般都将系统的一部分内存做为显存，这部分显存的大小由System BIOS指定，出于安全考虑，被占用的这部分内存对与CPU来说一般为不可见的，换据话说BIOS会将这段内存设置为reserve状态，并通过e820等方式报告给系统，系统不能将这段内存作为它用 dGPU类型的显卡都配有高带宽的显存，主流显存的大小为6G, 8G, 12G等，dGPU一般的通过PCIE的方式与系统相连，在32 bit系统上，因为整个物理地址空间只有4G，所以对于大容量的显存，并不能把全部的显存都映射到CPU的地址空间中供CPU访问，一般只有256M的大小的显存能被CPU直接访问，称这部分内存为visible显存，其余的部分为invisble 可以通过下面的命令查看显卡的硬件信息 12345678910111213$ lspci -vvvnnn -s 2f:00.02f:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Navi 14 [Radeon RX 5500/5500M / Pro 5500M] [1002:7340] (rev c5) (prog-if 00 [VGA controller]) Subsystem: Advanced Micro Devices, Inc. [AMD/ATI] Navi 14 [Radeon RX 5500/5500M / Pro 5500M] [1002:0b0c] Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx- Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx- Latency: 0, Cache Line Size: 64 bytes Interrupt: pin A routed to IRQ 255 Region 0: Memory at d0000000 (64-bit, prefetchable) [size=256M] Region 2: Memory at e0000000 (64-bit, prefetchable) [size=2M] Region 4: I/O ports at f000 [disabled] [size=256] Region 5: Memory at fce00000 (32-bit, non-prefetchable) [size=512K] Expansion ROM at fce80000 [disabled] [size=128K] Capabilities: &lt;access denied&gt; Region 0 是显存的MMIO空间，在没有打开Large Bar功能的时候，显存会被映射到CPU的低4G地址空间，并且最多只有256M。 Region 2 是Doorbell的MMIO空间，用于实现软件通知硬件的一种方式，值得注意的是，这部分空间是prefetchable 的，并且可以映射到用户空间 Region 4 是显卡的IO空间 Region 5 是显卡寄存器的MMIO空间，只有512K大小，并且是non-prefetchable Expansion ROM 是显卡的BIOS在image在存储域镜像的映射，属于Option ROM是PCI-E可选的一种功能 在64 bit 系统上，由于CPU看到的地址范围很大，可以把显卡的全部显存映射到CPU存储域地址空间供CPU直接访问，默认情况下这个功能默认是关闭的， 可以在System BIOS中打开这部分功能： Above 4G 可以把MMIO空间映射到系统4G空间以上 BAR Resize 可以在BIOS里完成BAR Resize，如果没有打开这个功能可以由GPU Driver完成Bar Resize动作 123456789101112$ lspci -vvvnnn -s 2f:00.02f:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Navi 14 [Radeon RX 5500/5500M / Pro 5500M] [1002:7340] (rev c5) (prog-if 00 [VGA controller]) Subsystem: Advanced Micro Devices, Inc. [AMD/ATI] Navi 14 [Radeon RX 5500/5500M / Pro 5500M] [1002:0b0c] Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+ Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx- Latency: 0, Cache Line Size: 64 bytes Interrupt: pin A routed to IRQ 83 Region 0: Memory at a00000000 (64-bit, prefetchable) [size=8G] Region 2: Memory at 900000000 (64-bit, prefetchable) [size=2M] Region 4: I/O ports at f000 [size=256] Region 5: Memory at fce00000 (32-bit, non-prefetchable) [size=512K] Expansion ROM at fce80000 [disabled] [size=128K] 在打开Above 4G 功能后 Region 0:a00000000 已经被重新映射到 4G以上的地址空间上 当在系统BIOS里打开上面提到的功能，我们可以把GPU的显存完整的映射到CPU的地址空间，这样所有内存都是VISIBILE 的了，下面是AMD GPU Driver做FB Bar Resize: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253int amdgpu_device_resize_fb_bar(struct amdgpu_device *adev){ u64 space_needed = roundup_pow_of_two(adev-&gt;gmc.real_vram_size); u32 rbar_size = order_base_2(((space_needed &gt;&gt; 20) | 1)) - 1; struct pci_bus *root; struct resource *res; ... /* Check if the root BUS has 64bit memory resources */ root = adev-&gt;pdev-&gt;bus; while (root-&gt;parent) root = root-&gt;parent; pci_bus_for_each_resource(root, res, i) { if (res &amp;&amp; res-&gt;flags &amp; (IORESOURCE_MEM | IORESOURCE_MEM_64) &amp;&amp; res-&gt;start &gt; 0x100000000ull) break; } /* Trying to resize is pointless without a root hub window above 4GB */ if (!res) return 0; /* Disable memory decoding while we change the BAR addresses and size */ pci_read_config_word(adev-&gt;pdev, PCI_COMMAND, &amp;cmd); pci_write_config_word(adev-&gt;pdev, PCI_COMMAND, cmd &amp; ~PCI_COMMAND_MEMORY); /* Free the VRAM and doorbell BAR, we most likely need to move both. */ amdgpu_device_doorbell_fini(adev); if (adev-&gt;asic_type &gt;= CHIP_BONAIRE) pci_release_resource(adev-&gt;pdev, 2); pci_release_resource(adev-&gt;pdev, 0); r = pci_resize_resource(adev-&gt;pdev, 0, rbar_size); if (r == -ENOSPC) DRM_INFO(&quot;Not enough PCI address space for a large BAR.&quot;); else if (r &amp;&amp; r != -ENOTSUPP) DRM_ERROR(&quot;Problem resizing BAR0 (%d).&quot;, r); pci_assign_unassigned_bus_resources(adev-&gt;pdev-&gt;bus); /* When the doorbell or fb BAR isn't available we have no chance of * using the device. */ r = amdgpu_device_doorbell_init(adev); if (r || (pci_resource_flags(adev-&gt;pdev, 0) &amp; IORESOURCE_UNSET)) return -ENODEV; pci_write_config_word(adev-&gt;pdev, PCI_COMMAND, cmd); return 0;} 读到这里我想你心中会有一个疑问： 对于invisbile部分的显存CPU是如何访问呢？ 答：对于invisbile的显存，CPU是没有办法直接访问的。因为现代的GPU都有DMA能力，可以访问系统层面的内存。对于GPU来说可以直接使用这部分内存，也可以将System的内存内容搬运到GPU的显存上去使用，因为系统内存带宽相对来说比较低，所以一般还是需要搬运到显存上进行计算的。 GTT 内存GTT是 graphics translation table的缩写，也叫做GART(graphics aperture remapping table)允许有DMA能力的GPU硬件将系统内存作为GPU显存。 GPU除了可以使用显卡上自带的独立显存，还可以使用系统的内存做为GPU的显存，不过有别与集成显卡的那部分显存，GTT内存是可以按需分配的，并不要求在开机的过程中就reserve一段空间给GPU。 因为系统的内存也很宝贵，并不能将全部的内存都作为GTT内存，在TTM实现里一般将系统内存的 1/2 可以供给外设使用，超过这部分内存TTMdriver会进行 swap操作来回收内存，避免系统发生OOM。值得注意的是： 整个计算机硬件里可以有多张显卡，且各个显卡的厂商也可能不一样，这1/2的内存将share给所有外设使用。 123456789101112131415161718192021222324252627282930static int ttm_mem_init_kernel_zone(struct ttm_mem_global *glob, const struct sysinfo *si){ struct ttm_mem_zone *zone = kzalloc(sizeof(*zone), GFP_KERNEL); uint64_t mem; int ret; if (unlikely(!zone)) return -ENOMEM; mem = si-&gt;totalram - si-&gt;totalhigh; mem *= si-&gt;mem_unit; zone-&gt;name = &quot;kernel&quot;; zone-&gt;zone_mem = mem; zone-&gt;max_mem = mem &gt;&gt; 1; zone-&gt;emer_mem = (mem &gt;&gt; 1) + (mem &gt;&gt; 2); zone-&gt;swap_limit = zone-&gt;max_mem - (mem &gt;&gt; 3); zone-&gt;used_mem = 0; zone-&gt;glob = glob; glob-&gt;zone_kernel = zone; ret = kobject_init_and_add( &amp;zone-&gt;kobj, &amp;ttm_mem_zone_kobj_type, &amp;glob-&gt;kobj, zone-&gt;name); if (unlikely(ret != 0)) { kobject_put(&amp;zone-&gt;kobj); return ret; } glob-&gt;zones[glob-&gt;num_zones++] = zone; return 0;} zone-&gt;max_mem 就是系统最多可以分给硬件内存的大小，这个大小并不是严格的系统内存1/2 zone-&gt;emer_mem 具有高级权限的进程可以额外的使用一部分内存 zone-&gt;swap_limit 当TTM回收内存到这个位置的时候，就停止了回收线程 这些参数主要作为TTM driver Water Mark,用来控制内存的回收时机，下面这个图可以帮你理解这些参数的作用：","link":"/2020/05/25/amdgpu/GPU_Physical_Memory/"},{"title":"AMD GPU VMID","text":"本文所讨论的均为AMD公司的显卡产品 显卡：AMD Radeon RX 5500 - 8GB 显存 Linux：Linux-5.4.y (fc944ddc0b4a) AMD GPU HUB在AMD GPU内部有很多种类的IP，这些不同种类的IP硬件一起构成了完整的GPU硬件，这些子IP在运行的时候也需要访问VRAM或者sysMem的资源，但是不同的是，这些IP并没有与实际的Memroy Controler(MC) 相连，而是汇总到一个称作为“HUB”的硬件单元上，由这个硬件单元代理进行统一的寻址，这个硬件和我们在计算机上看到南桥(PCH: Platfrom Controller Hub)很类似，计算机将一些低速的设备（NIC, SATA, USB ..) 连接到南桥，再由南桥芯片统一和CPU进行通信。 由于GPU硬件设计的需要，在AMD GPU内部集成了一个或者多个HUB硬件，系统将不同种类的IP硬件连接到不同的HUB上，由HUB代理IP进行IO访问，GPU内部IP大多数设备都是基于HUB来进行内存访问的。 HUB IP Note GFX HUB （1个） GC （Graphic Core), sDMA 主要计算单元: SE/SH/CU/SIMD MM HUB （1个或者多个） UVD, VCE, VCN, JPEG, DCN 多媒体: 编码解码等IP GPU HUB 功能GPU HUB主要由以下几种功能： 地址翻译：将GPU的虚拟地址翻译成总线地址 TLB：Translation Lookaside Buffer 提供了一个Cache硬件用于加速地址翻译 AMD GPU硬件提供了2种地址翻译机制： GPU VM：由GPU自身完成地址翻译，并将翻译的结果直接送到地址总线进行寻址。(主要模式) ATC/ATS：由IOMMU提供地址翻译过程，并将翻译结果缓存到ATC中。（APU) ATS: Address Translation Services 基于PCIE协议的ATS服务， 需要搭配IOMMU使用 ATC: Address Translation Cache 用于缓存地址翻译结果（aka IO-TLB） VMID (Virtual Memory/Machine Identity)由于HUB自身提供了地址翻译功能，那么上游的IP就可以用虚拟的GPU地址用于内存访问，简化了编程门槛，同时提隔离了不同VM的虚拟地址空间。 AMD GPU提供了为每个HUB提供了16份VMID，意味着每个HUB在统一时刻可以表示16份不同的地址空间，软件在提交Command的时候需要将自己VM的GPU虚拟地址空间绑定到一个VMID上，由这个VMID来表示GPU虚拟地址空间的layout，通俗来讲GPU有16个MMU单元可以同时地址翻译功能。 尽管每个HUB由16个VMID，但是在硬件设计的时候VMID-0是比较特殊的： GART 地址空间在VMID-0上 KMD提交Command是在VMID-0上 VMID-0提供System Aperature概念，简化内核态的物理地址翻译过程 VMID 分配在Linux平台上VMID按照如下用途进行分配 VMID Use 0 GART Kernel Submit Command 1 - 7 User 3D Application (/dev/card0, render128) opengl, vulkan, 等3D程序 8 - 15 KFD Application (/dev/kfd) Hip,Rocm等程序 VMID 虚拟地址空间GPU虚拟地址范围本文所讨论的AMD Radeon RX 5500显卡是一个64位的GPU，但是实际的虚拟地址范围是48bit，也就是寻址能力是2^48。 AMD 的GPU虚拟地址范围会影响GPU页表的翻译级数，随着地址宽度的 增加，翻译级数也需要随着增加，否则会浪费更多的页表内存，页表级数的增加意味着翻译效率的降低，在AMD GPU kernel Driver中，GPU VM的虚拟地址范围由 System Memory 和 VRAM的大小计算出来的。 计算方法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445void amdgpu_vm_adjust_size(struct amdgpu_device *adev, uint32_t min_vm_size, uint32_t fragment_size_default, unsigned max_level, unsigned max_bits){ unsigned int max_size = 1 &lt;&lt; (max_bits - 30); unsigned int vm_size; uint64_t tmp; /* adjust vm size first */ if (amdgpu_vm_size != -1) { vm_size = amdgpu_vm_size; if (vm_size &gt; max_size) { dev_warn(adev-&gt;dev, &quot;VM size (%d) too large, max is %u GB\\n&quot;, amdgpu_vm_size, max_size); vm_size = max_size; } } else { struct sysinfo si; unsigned int phys_ram_gb; /* Optimal VM size depends on the amount of physical * RAM available. Underlying requirements and * assumptions: * * - Need to map system memory and VRAM from all GPUs * - VRAM from other GPUs not known here * - Assume VRAM &lt;= system memory * - On GFX8 and older, VM space can be segmented for * different MTYPEs * - Need to allow room for fragmentation, guard pages etc. * * This adds up to a rough guess of system memory x3. * Round up to power of two to maximize the available * VM size with the given page table size. */ si_meminfo(&amp;si); phys_ram_gb = ((uint64_t)si.totalram * si.mem_unit + (1 &lt;&lt; 30) - 1) &gt;&gt; 30; vm_size = roundup_pow_of_two( min(max(phys_ram_gb * 3, min_vm_size), max_size)); } adev-&gt;vm_manager.max_pfn = (uint64_t)vm_size &lt;&lt; 18; ...} 另外你可以在kernel的log里看到如下信息 1[ 65.752676] [drm] vm size is 262144 GB, 4 levels, block size is 9-bit, fragment size is 9-bit 其中 262144 GB (= 0x1000000000000） 就是KMD 计算出来的VM SIZE，这个大小会通过ioctl命令report给libdrm，供libdrm va-range分配器使用。 GPU 页表级数AMD GPU硬件支持 4 + 1级页表映射，一共最多可实现5级映射， +1级映射会影响TLB效率一般不使用。 AMD GPU Page Size 固定为4K 大小。 GPU页表翻译的时候使用的是物理地址翻译，但是GPU_PAGE_SIZE 为4k，当前的驱动并不能保证一定分配出多个连续的page作为PTE或者PDE。那么一个4K的物理配置能装多少个PTE呢？ 1 PTE = 8 Bytes1 PAGE = 4 KCount = 4K / 8 = 512 PTEs = 2 ^ 9 PTEs 通过上面的公式可以算出来，一个4k的物理page可以装下512个页表项，同理一个4k的物理page也可以装下512个目录项，所以对于4级页表一般按照如下分配： 64 VA: 9 - 9 - 9 - 9 - 12 32 VA : 10 - 10 - 10 -12 VMID的使用GPU在提交Command到GPU上，必须获得一个可以使用的VMID，并把自己的页表基地址设置到对应的VMID寄存器上，当前VMID才能表示当前GPU进程的GPU虚拟地址空间。后面的shader，纹理，fence等GPU资源才能正常工作。 VMID作为GPU硬件资源他的数量是有限的，当多个GPU进程同时运行时，需要分时的复用这些VMID资源，才能保证不同GPU进程工作正常。 由于VMID自身硬件自身的Cache（TLB）存有地址翻译的结果，所以当GPU进程切换的时候务必需要进行Cache Flush操作，这样不可避免的会造成性能的浪费，后面的文章我会详细描述一下 VMID 的分配算法。 总结希望这篇文章对你理解GPU VM有所帮助，错误的地方请联系作者改正。","link":"/2020/10/25/amdgpu/AMD_GPU_VMID_HW/"},{"title":"编译Linux内核","text":"环境准备1$ sudo apt install libelf-dev libssl-dev build-essential flex bison 下载内核源码1$ git clone https://mirrors.tuna.tsinghua.edu.cn/git/linux.git or 1$ git clone https://mirrors.tuna.tsinghua.edu.cn/git/linux-stable.git 在这里我们使用国内的清华tuna源加速下载内核源码 配置内核在这里我们使用ubuntu 20.04自己带的配置文件 1cp /usr/src/linux-headers-5.4.0-24-generic/.config arch/x86/configs/ubuntu_defconfig 编译内核生成配置文件1make -j$(nproc) O=../build/linux-kernel ubuntu_defconfig -j$(nproc) 启动多少个job来编译内核，nproc返回逻辑CPU的数量 O=../build/linux-kernel 在编译过程中会生成大量的.o文件，为了避免和源码混淆，指定输出目录 ubuntu_defconfig 是我们要指定的编译文件配置文件，位于arch/{ARCH}/configs/xxx_defconfig 如果一切都顺利的话，会在../build/linux-kernel目录下生成.config文件，这个将是这次编译内核所使用的配置文件 同时还会生成下面几个重要的配置文件 include/generated/autoconf.h 该文件会被所有内核源码包含，里面包含了大量的配置宏定义 123456789101112/* * * Automatically generated file; DO NOT EDIT. * Linux/x86 5.4.100-custom Kernel Configuration * */#define CONFIG_IP6_NF_MATCH_AH_MODULE 1#define CONFIG_NLS_CODEPAGE_861_MODULE 1#define CONFIG_MTD_SPI_NAND_MODULE 1#define CONFIG_RING_BUFFER 1#define CONFIG_HARDENED_USERCOPY_FALLBACK 1... 编译linux内核编译vmlinux， vmlinux是内核最终生成的内核文件 1make -j$(nproc) O=../build/linux-kernel vmlinux 也可以直接编译 整个deb包 1make -j$(nproc) O=../build/linux-kernel bindeb-pkg 编译bindeb-pkg需要编译很多内核模块，需要花费很多时间 编译内核模块1make -j$(nproc) O=../build/linux-kernel modules M=`pwd`/drivers/gpu/drm modules 该target用与编译内核模块 M=drivers/gpu/drm 用于指定编译的内核模块 1make -j$(nproc) O=../build/linux-kernel drivers/gpu/drm/amd/amdgpu/amdgpu.ko 也可以直接编译具体ko文件 重新生成配置文件如果修改了.config文件，需要重新输入下面命令来生成对应的头文件 1make -j$(nproc) O=../build/linux-kernel oldconfig 常用命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140Cleaning targets: clean - Remove most generated files but keep the config and enough build support to build external modules mrproper - Remove all generated files + config + various backup files distclean - mrproper + remove editor backup and patch filesConfiguration targets: config - Update current config utilising a line-oriented program nconfig - Update current config utilising a ncurses menu based program menuconfig - Update current config utilising a menu based program xconfig - Update current config utilising a Qt based front-end gconfig - Update current config utilising a GTK+ based front-end oldconfig - Update current config utilising a provided .config as base localmodconfig - Update current config disabling modules not loaded localyesconfig - Update current config converting local mods to core defconfig - New config with default from ARCH supplied defconfig savedefconfig - Save current config as ./defconfig (minimal config) allnoconfig - New config where all options are answered with no allyesconfig - New config where all options are accepted with yes allmodconfig - New config selecting modules when possible alldefconfig - New config with all symbols set to default randconfig - New config with random answer to all options listnewconfig - List new options olddefconfig - Same as oldconfig but sets new symbols to their default value without prompting kvmconfig - Enable additional options for kvm guest kernel support xenconfig - Enable additional options for xen dom0 and guest kernel support tinyconfig - Configure the tiniest possible kernel testconfig - Run Kconfig unit tests (requires python3 and pytest)Other generic targets: all - Build all targets marked with [*]* vmlinux - Build the bare kernel* modules - Build all modules modules_install - Install all modules to INSTALL_MOD_PATH (default: /) dir/ - Build all files in dir and below dir/file.[ois] - Build specified target only dir/file.ll - Build the LLVM assembly file (requires compiler support for LLVM assembly generation) dir/file.lst - Build specified mixed source/assembly target only (requires a recent binutils and recent build (System.map)) dir/file.ko - Build module including final link modules_prepare - Set up for building external modules tags/TAGS - Generate tags file for editors cscope - Generate cscope index gtags - Generate GNU GLOBAL index kernelrelease - Output the release version string (use with make -s) kernelversion - Output the version stored in Makefile (use with make -s) image_name - Output the image name (use with make -s) headers_install - Install sanitised kernel headers to INSTALL_HDR_PATH (default: ./usr)Static analysers: checkstack - Generate a list of stack hogs namespacecheck - Name space analysis on compiled kernel versioncheck - Sanity check on version.h usage includecheck - Check for duplicate included header files export_report - List the usages of all exported symbols headerdep - Detect inclusion cycles in headers coccicheck - Check with CoccinelleTools: nsdeps - Generate missing symbol namespace dependenciesKernel selftest: kselftest - Build and run kernel selftest (run as root) Build, install, and boot kernel before running kselftest on it kselftest-clean - Remove all generated kselftest files kselftest-merge - Merge all the config dependencies of kselftest to existing .config.Userspace tools targets: use &quot;make tools/help&quot; or &quot;cd tools; make help&quot;Kernel packaging: rpm-pkg - Build both source and binary RPM kernel packages binrpm-pkg - Build only the binary kernel RPM package deb-pkg - Build both source and binary deb kernel packages bindeb-pkg - Build only the binary kernel deb package snap-pkg - Build only the binary kernel snap package (will connect to external hosts) tar-pkg - Build the kernel as an uncompressed tarball targz-pkg - Build the kernel as a gzip compressed tarball tarbz2-pkg - Build the kernel as a bzip2 compressed tarball tarxz-pkg - Build the kernel as a xz compressed tarball perf-tar-src-pkg - Build perf-5.4.100.tar source tarball perf-targz-src-pkg - Build perf-5.4.100.tar.gz source tarball perf-tarbz2-src-pkg - Build perf-5.4.100.tar.bz2 source tarball perf-tarxz-src-pkg - Build perf-5.4.100.tar.xz source tarballDocumentation targets: Linux kernel internal documentation in different formats from ReST: htmldocs - HTML latexdocs - LaTeX pdfdocs - PDF epubdocs - EPUB xmldocs - XML linkcheckdocs - check for broken external links (will connect to external hosts) refcheckdocs - check for references to non-existing files under Documentation cleandocs - clean all generated files make SPHINXDIRS=&quot;s1 s2&quot; [target] Generate only docs of folder s1, s2 valid values for SPHINXDIRS are: make SPHINX_CONF={conf-file} [target] use *additional* sphinx-build configuration. This is e.g. useful to build with nit-picking config. Default location for the generated documents is Documentation/outputArchitecture specific targets (x86):* bzImage - Compressed kernel image (arch/x86/boot/bzImage) install - Install kernel using (your) ~/bin/installkernel or (distribution) /sbin/installkernel or install to $(INSTALL_PATH) and run lilo fdimage - Create 1.4MB boot floppy image (arch/x86/boot/fdimage) fdimage144 - Create 1.4MB boot floppy image (arch/x86/boot/fdimage) fdimage288 - Create 2.8MB boot floppy image (arch/x86/boot/fdimage) isoimage - Create a boot CD-ROM image (arch/x86/boot/image.iso) bzdisk/fdimage*/isoimage also accept: FDARGS=&quot;...&quot; arguments for the booted kernel FDINITRD=file initrd for the booted kernel i386_defconfig - Build for i386 ubuntu_defconfig - Build for ubuntu x86_64_defconfig - Build for x86_64 make V=0|1 [targets] 0 =&gt; quiet build (default), 1 =&gt; verbose build make V=2 [targets] 2 =&gt; give reason for rebuild of target make O=dir [targets] Locate all output files in &quot;dir&quot;, including .config make C=1 [targets] Check re-compiled c source with $CHECK (sparse by default) make C=2 [targets] Force check of all c source with $CHECK make RECORDMCOUNT_WARN=1 [targets] Warn about ignored mcount sections make W=n [targets] Enable extra build checks, n=1,2,3 where 1: warnings which may be relevant and do not occur too often 2: warnings which occur quite often but may still be relevant 3: more obscure warnings, can most likely be ignored Multiple levels can be combined with W=12 or W=123","link":"/2020/05/25/linux/buildLinuxKernel/"},{"title":"AMD IOMMU 介绍","text":"主要功能 I/O DMA 访问权限检查和地址翻译 可选：支持用户VM的地址翻译 一个Device Table可以允许 I/O设备被分配给以个指定的domain并且包含一个指针指向Device Page Table 中断重映射并提供权限检查 可选：支持AMD64 vAPIC 基于内存的命令和状态交互在处理器和iommu设备之间 可选：支持 Peripheral Page request (PPR) Log 可以减轻PPR 和 Event Log Overflow 可选：基于硬件的允许特权I/O设备访问预定义的系统内存 系统拓扑图 使用模型 替换GART（Graphic Access Remote Table） 替换DEV （Device Exclusion Vector) 对32bit设备访问64bit控制提供支持 用户态设备的直接访问 虚拟机访问设备支持 虚拟的IOMMU (vIOMMU) 虚拟化的用户态设备访问支持 IOMMU可选功能支持 Guest virtual to guest physical address translation capability AMD64 long mode page table compatibility Support for PCI ATS Support for PCI-SIG PRI and PASID TLP prefix ECN Support for a guest virtual APIC (e.g., AVIC) Enhanced performance and error logging features Guest page table User/Supervisor access privilege checking Guest page table Global Supervisor-level access protection Guest page table non-executable page protection Segmentation of the Device Table PPR and Event Log dual buffers with optional autoswap PPR Auto Response with Always-on feature PPR Log early overflow warning Device-specific feature reporting registers MMIO access to MSI setup and mapping configuration space fields Memory Access Routing and Control (MARC) Automatic Block StopMark Message Handling Guest I/O Protection x2APIC Hardware Accelerated Virtualized IOMMU (vIOMMU) Secure ATS Secure Nested Paging","link":"/2021/10/20/iommu/amd-iommu-1/"},{"title":"Amdgpu Monitor","text":"AMDTOP 个人项目：C语言编写，基于TUI，实时显示AMD GPU 显卡状态 amdtop 可以实时显示AMD GPU的状态，喜欢的可以点击 amdtop 下载。 功能介绍 实时显示GPU频率 实时显示GPU 显存使用情况 实时显示CPU / GPU负载 实时显示GPU功率信息 实时显示使用GPU的进程 显示显卡型号： PCIE Bus, Family 显示驱动信息 显示BIOS信息 显示DISPLAY信息：Monitor数量，分辨率，layout等… 显示Ring, Fence, Firmware等调试信息 支持Multi-GPU 基于TUI 自适应终端窗口大小 下载和编译git clone1git clone git@github.com:xxlnx/amdtop.git amdtop 压缩包下载amdtop下载 编译123$ make build &amp;&amp; cd build$ cmake ../ -DCMAKE_BUILD_TYPE=Release$ make -j8 debug 版本： cmake ../ -DCMAKE_BUILD_TYPE=Debug 编译deb包123$ make build &amp;&amp; cd build$ cmake ../ -DCMAKE_BUILD_TYPE=Release$ make -j8 package 安装1$ dpkg -i amdtop-0.1.0-Linux.deb 截图 关于代码完全开源免费。","link":"/2019/09/25/tool/amdtop/"},{"title":"AMD GPU Ring Buffer","text":"本文所讨论的均为AMD公司的显卡产品 显卡：AMD Radeon RX 5500 - 8GB 显存 Linux：Linux-5.4.y (fc944ddc0b4a) 背景现代的驱动模型中，一般有2种方式来操作硬件： PUSH：CPU通过读写寄存器的方式来控制GPU硬件。 PULL：CPU将命令写到一块buffer中，并通知硬件，由硬件来解析命令并执行。 2种模式各有优劣，使用PUSH的方式一般都为同步操作，需要占用CPU时间，并且在GPU领域有大量的寄存器是不方便用户直接读写的，所以GPU大多数采用的是PULL方式。因此本文主要介绍AMD KMD Ring Buffer的原理和实现。 AMD GPU Ring Buffer 在软件领域，生产消费模型是一个典型的软件模型，一端负责生产数据，另一端负责消费数据，两端通过某种规则建立一种数据通道来达到通信的目的，同样的在GPU领域，CPU作为生产者，产生命令，GPU作为消费者，解析并执行命令，因此CPU和GPU就完成了通信。 在KMD驱动中，CPU通过一个Ring Buffer的软件模型和硬件进行通信，Ring Buffer就是一个环形缓冲区，内部有wptr和rptr来维护Ring的状态，在CPU写入数据的时候CPU会更新wptr，当GPU消费数据后会更新rptr， 因此GPU的状态可以大致分为2种： rptr == wptr：Ring Buffer为空，GPU没有命令需要处理 rptr != wptr： Ring Buffer有数据，GPU解析并执行命令 Field Description Note Buffer Base 唤醒缓冲区的地址 CPU/GPU都可以访问这个唤醒缓冲区 Buffer Size 唤醒缓冲区的大小 约定缓冲区的大小 Writer Pointer 写指针 CPU发送命令的时候更新wptr Read Pointer 读指针 GPU处理完命令后更新rptr 在 AMD GPU内部，硬件会根据Engine的类型提供不同数量的Queue（Ring）和软件进行交互，相同Engine下的不同Queue会根据配置具有不同的硬件能力： Engine Ring Count Note GFX 1 - 2~ 3D Graphcis： 3D and HP3D Compute 32~ 数学/矩阵计算 SDMA (1 - 7) * 10~ System DMA 数据拷贝/填充/搬运… UVD 1~ Unified Video Decode VCE 1~ Video Encode Engine AMD GPU Ring 命令类型不同Engine定义了不同类型的命令，其中最主要的GFX/Compute Engined定义了2种命令类型： PM4：AMD GPU内部主要的命令类型。 PM4 AQL (Architected Queuing Language)： HSA 组织定义的命令类型 (Compute Only)。 HSA-AQL PM4: Packet 0 Type-0 Packets are discouraged, but can be used if absolutely required. Type-3 packets should be used instead. Write N DWords in the information body to the N consecutive registers, or to the register, pointed to by the BASE_INDEX field of the packet header. Does check for context roll. This packet supports a register memory map up to 64K DWords (256K Bytes). PM4: Packet 1 Not Support. PM4: Packet 2 This is a filler packet. It has only the header, and its content is not important except for bits 30 and 31. It is used to fill up the trailing space left when the allocated buffer for a packet, or packets, is not fully filled. This allows the CP to skip the trailing space and to fetch the next packet. PM4: Packet 3 Type-3 packets have a common format for their headers. However, the size of their information body may vary depending on the value of field IT_OPCODE. The size of the information body is indicated by field COUNT. If the size of the information is N DWords, the value of COUNT is N-1. In the following packet definitions, we will describe the field IT_BODY for each packet with respect to a given IT_OPCODE, and omit the header. . Packet3 是现在GPU支持最多的命令类型 HEADER: TYPE: 3 COUNT: IT_BODY 长度 - 1 IT_OPCODE: OP 操作码，用于表示具体的命令类型 SHADER_TYPE: GFX 或者 Compute IT_BOBDY：根据IT_OPCODE不同，填写不同的命令内容 PKT3 IT_OPCODE 列表12345678910111213141516171819202122232425262728293031323334353637383940414243drivers/gpu/drm/amd/amdgpu/soc15d.h#define PACKET3(op, n) ((PACKET_TYPE3 &lt;&lt; 30) | \\ (((op) &amp; 0xFF) &lt;&lt; 8) | \\ ((n) &amp; 0x3FFF) &lt;&lt; 16)#define PACKET3_COMPUTE(op, n) (PACKET3(op, n) | 1 &lt;&lt; 1).... /* Packet 3 types */#define PACKET3_NOP 0x10#define PACKET3_SET_BASE 0x11#define PACKET3_BASE_INDEX(x) ((x) &lt;&lt; 0)#define CE_PARTITION_BASE 3#define PACKET3_CLEAR_STATE 0x12#define PACKET3_INDEX_BUFFER_SIZE 0x13#define PACKET3_DISPATCH_DIRECT 0x15#define PACKET3_DISPATCH_INDIRECT 0x16#define PACKET3_ATOMIC_GDS 0x1D#define PACKET3_ATOMIC_MEM 0x1E....#define PACKET3_STRMOUT_BUFFER_UPDATE 0x34```#define PACKET3(op, n) ((PACKET_TYPE3 &lt;&lt; 30) | \\ (((op) &amp; 0xFF) &lt;&lt; 8) | \\ ((n) &amp; 0x3FFF) &lt;&lt; 16)#define PACKET3_COMPUTE(op, n) (PACKET3(op, n) | 1 &lt;&lt; 1).... /* Packet 3 types */#define PACKET3_NOP 0x10#define PACKET3_SET_BASE 0x11#define PACKET3_BASE_INDEX(x) ((x) &lt;&lt; 0)#define CE_PARTITION_BASE 3#define PACKET3_CLEAR_STATE 0x12#define PACKET3_INDEX_BUFFER_SIZE 0x13#define PACKET3_DISPATCH_DIRECT 0x15#define PACKET3_DISPATCH_INDIRECT 0x16#define PACKET3_ATOMIC_GDS 0x1D#define PACKET3_ATOMIC_MEM 0x1E....#define PACKET3_STRMOUT_BUFFER_UPDATE 0x34 PKT3 例子Linux AMDGPU Driver在初始化完硬件后，会进行一个简单的Ring Test： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455static int gfx_v10_0_ring_test_ring(struct amdgpu_ring *ring){ struct amdgpu_device *adev = ring-&gt;adev; uint32_t scratch; uint32_t tmp = 0; unsigned i; int r; r = amdgpu_gfx_scratch_get(adev, &amp;scratch); /* 0 */ if (r) { DRM_ERROR(&quot;amdgpu: cp failed to get scratch reg (%d).\\n&quot;, r); return r; } WREG32(scratch, 0xCAFEDEAD); /* 1 */ r = amdgpu_ring_alloc(ring, 3); /* 2 */ if (r) { DRM_ERROR(&quot;amdgpu: cp failed to lock ring %d (%d).\\n&quot;, ring-&gt;idx, r); amdgpu_gfx_scratch_free(adev, scratch); return r; } amdgpu_ring_write(ring, PACKET3(PACKET3_SET_UCONFIG_REG, 1)); /* 3 */ amdgpu_ring_write(ring, (scratch - PACKET3_SET_UCONFIG_REG_START)); amdgpu_ring_write(ring, 0xDEADBEEF); amdgpu_ring_commit(ring); /* 4 */ for (i = 0; i &lt; adev-&gt;usec_timeout; i++) { tmp = RREG32(scratch); /* 5 */ if (tmp == 0xDEADBEEF) break; if (amdgpu_emu_mode == 1) msleep(1); else udelay(1); } if (i &lt; adev-&gt;usec_timeout) { if (amdgpu_emu_mode == 1) DRM_INFO(&quot;ring test on %d succeeded in %d msecs\\n&quot;, ring-&gt;idx, i); else DRM_INFO(&quot;ring test on %d succeeded in %d usecs\\n&quot;, ring-&gt;idx, i); } else { DRM_ERROR(&quot;amdgpu: ring %d test failed (scratch(0x%04X)=0x%08X)\\n&quot;, ring-&gt;idx, scratch, tmp); r = -EINVAL; } amdgpu_gfx_scratch_free(adev, scratch); /* 6 */ return r;} 命令解析： 分配一个scrathch寄存器 使用CPU将其初始化为 0xCAFEDEAD 在Ring Buffer里分配一个 3 + 1 长度空间 填写命令头 PACKET3_SET_UCONFIG_REG： 命令类型，设置某一个寄存器的值 1： IT_BODY 长度为 1 + 1 scratch - PACKET3_SET_UCONFIG_REG_START： 寄存器地址 0xDEADBEEF：寄存器值 提交命令（更新wptr）到GPU 检查scratch寄存器的内容是否有变化： 如果寄存器内容从 0xCAFEDEAD变化成 0xDEADEEF则表示命令执行成功，Ring工作正常 AMD GPU Inidirect Command BufferIndirect Command Buffer又称为IB，是AMD GPU最常用的PKT3类型的命令包，主要功能就是间接的执行一组Command，只需要将Command所在的地址设置到命令包中即可，这种方式非常像编程里的函数调用，但是由于硬件限制，仅可以被调用一次，不可以嵌套执行。 PKT3: INDIRECT BUFFER HEADER: 包含命令类型，长度，OP等信息 IB_BASE_LO | IB_BASE_HI: Command包在GPU视角下的地址 VMID： 因为命令包可以从不同VMID下发送下来，需要指定VMID才可以根据IB地址确定最终的物理地址 IB_SIZE： Command包的长度 PKT3: INDIRECT BUFFER 例子AMD GPU KMD驱动在初始化结束的时候，同样会做一个简单的IB 测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960static int gfx_v10_0_ring_test_ib(struct amdgpu_ring *ring, long timeout){ struct amdgpu_device *adev = ring-&gt;adev; struct amdgpu_ib ib; struct dma_fence *f = NULL; uint32_t scratch; uint32_t tmp = 0; long r; r = amdgpu_gfx_scratch_get(adev, &amp;scratch); /* 0 */ if (r) { DRM_ERROR(&quot;amdgpu: failed to get scratch reg (%ld).\\n&quot;, r); return r; } WREG32(scratch, 0xCAFEDEAD); /* 1 */ memset(&amp;ib, 0, sizeof(ib)); r = amdgpu_ib_get(adev, NULL, 256, &amp;ib); /* 2 */ if (r) { DRM_ERROR(&quot;amdgpu: failed to get ib (%ld).\\n&quot;, r); goto err1; } ib.ptr[0] = PACKET3(PACKET3_SET_UCONFIG_REG, 1); /* 3 */ ib.ptr[1] = ((scratch - PACKET3_SET_UCONFIG_REG_START)); ib.ptr[2] = 0xDEADBEEF; ib.length_dw = 3; r = amdgpu_ib_schedule(ring, 1, &amp;ib, NULL, &amp;f); /* 4 */ if (r) goto err2; r = dma_fence_wait_timeout(f, false, timeout); /* 5 */ if (r == 0) { DRM_ERROR(&quot;amdgpu: IB test timed out.\\n&quot;); r = -ETIMEDOUT; goto err2; } else if (r &lt; 0) { DRM_ERROR(&quot;amdgpu: fence wait failed (%ld).\\n&quot;, r); goto err2; } tmp = RREG32(scratch); /* 6 */ if (tmp == 0xDEADBEEF) { DRM_INFO(&quot;ib test on ring %d succeeded\\n&quot;, ring-&gt;idx); r = 0; } else { DRM_ERROR(&quot;amdgpu: ib test failed (scratch(0x%04X)=0x%08X)\\n&quot;, scratch, tmp); r = -EINVAL; }err2: amdgpu_ib_free(adev, &amp;ib, NULL); dma_fence_put(f);err1: amdgpu_gfx_scratch_free(adev, scratch); return r;} 这个IB 测试和之前的Ring Test一样，都是改写某一个寄存器： 分配一个scratch寄存器 CPU修改寄存器值为0xCAFEDEAD 2为IB分配一块buffer 填充IB内容,内容为PACKET3_SET_UCONFIG_REG去设置寄存器 提交IB到GPU SHCEDULER，等待调度 等待IB执行完成 CPU检查scratch是否成功设置为0xdeadbeef AMD GPU HW Queue Type PQ (Primary Queue): 主要的AMD GPU Queue IB(Indirect Buffer): Indirect Buffer Queue IQ(Interrupt Queue): 被挂起的Queue EOP(End of Pipe): 用于标识Command执行完毕 其中PQ 和 IB 是比较重要的2个Queue. AMD GPU Ring ControlRing Buffer是软件和GPU硬件沟通的主要通道，其中wptr和rptr是两个非常重要的信息，那么软件和GPU是如何交互的呢？ KMD驱动会为每一个ring分配2个DWORD分别用于存取rptr和wptr： 1234567891011121314151617r = amdgpu_device_wb_get(adev, &amp;ring-&gt;rptr_offs);if (r) { dev_err(adev-&gt;dev, &quot;(%d) ring rptr_offs wb alloc failed\\n&quot;, r); return r;}r = amdgpu_device_wb_get(adev, &amp;ring-&gt;wptr_offs);if (r) { dev_err(adev-&gt;dev, &quot;(%d) ring wptr_offs wb alloc failed\\n&quot;, r); return r;}r = amdgpu_device_wb_get(adev, &amp;ring-&gt;fence_offs);if (r) { dev_err(adev-&gt;dev, &quot;(%d) ring fence_offs wb alloc failed\\n&quot;, r); return r;} 这2个DWORD所占用的内存 CPU 和 GPU 都可以直接访问，这样双方就可以通过共享内存的方式进行通信，但是GPU去轮寻wptr是否变化效率很低，因此GPU硬件还提供下面集中方式来更新wptr： 操作寄存器 ：通过操作专用寄存器的方式来通知GPU硬件，寄存器有权限问题，不使用于User Mode Queue。 Doorbell：使用Doorbell机制来通知GPU硬件。 doorbell 共享内存（硬件轮寻）： 让GPU去轮寻某一个地址上的值是否更新，效率低。 Doorbell123456789101112132f:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Navi 14 [Radeon RX 5500/5500M / Pro 5500M] [1002:7340] (rev c5) (prog-if 00 [VGA controller]) Subsystem: Advanced Micro Devices, Inc. [AMD/ATI] Navi 14 [Radeon RX 5500/5500M / Pro 5500M] [1002:0b0c] Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+ Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx- Latency: 0, Cache Line Size: 64 bytes Interrupt: pin A routed to IRQ 83 Region 0: Memory at d0000000 (64-bit, prefetchable) [size=256M] Region 2: Memory at e0000000 (64-bit, prefetchable) [size=2M] Region 4: I/O ports at f000 [size=256] Region 5: Memory at fce00000 (32-bit, non-prefetchable) [size=512K] Expansion ROM at fce80000 [disabled] [size=128K] Capabilities: &lt;access denied&gt; Kernel driver in use: amdgpu Region 2： GPU 硬件提供 2M 空间的Doorbell寄存器 AMD GPU KMD驱动动态来的来管理这些Doorbell寄存器，这些寄存器可以动态的和某一个Ring进行关联，只需要将DoorBell 寄存器的Index信息设置到对应的寄存器中即可，软件就可以通过操作doorbell来达到更新wptr的目的。 doorbell的分配要符合硬件限制。 Example: GFX 更新WPTR12345678910111213static void gfx_v10_0_ring_set_wptr_gfx(struct amdgpu_ring *ring){ struct amdgpu_device *adev = ring-&gt;adev; if (ring-&gt;use_doorbell) { /* XXX check if swapping is necessary on BE */ atomic64_set((atomic64_t *)&amp;adev-&gt;wb.wb[ring-&gt;wptr_offs], ring-&gt;wptr); WDOORBELL64(ring-&gt;doorbell_index, ring-&gt;wptr); } else { WREG32_SOC15(GC, 0, mmCP_RB0_WPTR, lower_32_bits(ring-&gt;wptr)); WREG32_SOC15(GC, 0, mmCP_RB0_WPTR_HI, upper_32_bits(ring-&gt;wptr)); }} 如果ring支持doorbell操作，就使用分配好的doorbell来更新wptr值 如果ring不支持doorbell，就使用更新寄存器的方式来更新wptr AMD GPU Ring HW FenceAMD GPU KMD在初始化ring的时候除了分配wptr 和 rptr还会分配一个fence的DWORD，用于接受GPU传递的信息。当CPU发送命令给GPU后，CPU有时候需要知道命令是否执行完毕，因此在必要的时候，CPU会在发送命令的时候在发送一个额外的packet命令包，这个命令包会在某一个地址上写入一个软件标记，并产生一个硬件中断来通知软件命令已经执行完毕。 Fence Emit PACKET3_RELEASE_MEM： 用于产生一个EOP事件，标识硬件已经执行完一组命令，可以释放一些硬件资源，并将一个软件标记写入到指定内存地址，并产生一个硬件中断来通知软件。 12345678910111213141516171819202122232425262728293031323334353637static void gfx_v10_0_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq, unsigned flags){ struct amdgpu_device *adev = ring-&gt;adev; bool write64bit = flags &amp; AMDGPU_FENCE_FLAG_64BIT; bool int_sel = flags &amp; AMDGPU_FENCE_FLAG_INT; /* Interrupt not work fine on GFX10.1 model yet. Use fallback instead */ if (adev-&gt;pdev-&gt;device == 0x50) int_sel = false; /* RELEASE_MEM - flush caches, send int */ amdgpu_ring_write(ring, PACKET3(PACKET3_RELEASE_MEM, 6)); /* 0 */ amdgpu_ring_write(ring, (PACKET3_RELEASE_MEM_GCR_SEQ | PACKET3_RELEASE_MEM_GCR_GL2_WB | PACKET3_RELEASE_MEM_GCR_GLM_INV | /* must be set with GLM_WB */ PACKET3_RELEASE_MEM_GCR_GLM_WB | PACKET3_RELEASE_MEM_CACHE_POLICY(3) | PACKET3_RELEASE_MEM_EVENT_TYPE(CACHE_FLUSH_AND_INV_TS_EVENT) | PACKET3_RELEASE_MEM_EVENT_INDEX(5))); amdgpu_ring_write(ring, (PACKET3_RELEASE_MEM_DATA_SEL(write64bit ? 2 : 1) | PACKET3_RELEASE_MEM_INT_SEL(int_sel ? 2 : 0))); /* 1 */ /* * the address should be Qword aligned if 64bit write, Dword * aligned if only send 32bit data low (discard data high) */ if (write64bit) BUG_ON(addr &amp; 0x7); else BUG_ON(addr &amp; 0x3); amdgpu_ring_write(ring, lower_32_bits(addr)); /* 2 */ amdgpu_ring_write(ring, upper_32_bits(addr)); amdgpu_ring_write(ring, lower_32_bits(seq)); /* 3 */ amdgpu_ring_write(ring, upper_32_bits(seq)); amdgpu_ring_write(ring, 0);} 初始化 PACKET3_RELEASE_MEM header 设置是否要产生硬件中断 设置标记地址 设置标记内容 因为在GPU硬件中，PKT3一般都是顺序执行的，这条PKT执行完毕后，会产生一个硬件终端，并标这条PKT之前的命令都已经执行完毕。 seq是软件维护的一个单调递增的数字标记，用于表示pkt执行的位置。（Per-Ring） 当硬件执行收到中断后，会在中断处理函数中释放fence，并按照dma-fence接口通知关心这个事件的人。 Fence Process当GPU收到硬件的中断后，会根据中断所提供的信息，路由到具体ring中，并执行amdgpu_fence_process()函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647bool amdgpu_fence_process(struct amdgpu_ring *ring){ struct amdgpu_fence_driver *drv = &amp;ring-&gt;fence_drv; uint32_t seq, last_seq; int r; do { last_seq = atomic_read(&amp;ring-&gt;fence_drv.last_seq); seq = amdgpu_fence_read(ring); } while (atomic_cmpxchg(&amp;drv-&gt;last_seq, last_seq, seq) != last_seq); /* 0 */ if (del_timer(&amp;ring-&gt;fence_drv.fallback_timer) &amp;&amp; seq != ring-&gt;fence_drv.sync_seq) amdgpu_fence_schedule_fallback(ring); /* 1 */ if (unlikely(seq == last_seq)) return false; last_seq &amp;= drv-&gt;num_fences_mask; seq &amp;= drv-&gt;num_fences_mask; do { struct dma_fence *fence, **ptr; ++last_seq; last_seq &amp;= drv-&gt;num_fences_mask; ptr = &amp;drv-&gt;fences[last_seq]; /* There is always exactly one thread signaling this fence slot */ fence = rcu_dereference_protected(*ptr, 1); RCU_INIT_POINTER(*ptr, NULL); if (!fence) continue; r = dma_fence_signal(fence); /* 2 */ if (!r) DMA_FENCE_TRACE(fence, &quot;signaled from irq context\\n&quot;); else BUG(); dma_fence_put(fence); } while (last_seq != seq); return true;} 因为IRQ的负载均衡的原因，中断可能被分配到不同的cpu上，这里采用类似读写锁的方式来保证各个cpu上处理的fence不会冲突 因为fence在发射的时候还会额外启动一个定时器，当定时器超时后，会强制处理fence，这里重新调整定时器延迟 使用dma_fence提供的signal接口，依次调用cb函数，来通知关心这个fence的人。 总结本文简单的介绍了一下KMD下实现的Ring Buffer，希望能对你有所帮助。","link":"/2020/10/25/amdgpu/AMD_GPU_Ring/"},{"title":"AMD GPU 虚拟内存","text":"本文所讨论的均为AMD公司的显卡产品 显卡：AMD Radeon RX 5500 - 8GB 显存 Linux：Linux-5.4.y (fc944ddc0b4a) Arch： X86_64 AMD GPU 虚拟内存现代的GPU内部都包含有一个类似MMU的内存管理单元，负责GPU侧的虚拟地址到物理的翻译。和CPU类似，GPU也有一个多级页表的数据结构来管理GPU页表的映射，每一个GPU进程有一份GPU页表，在提交Command的时候GPU驱动会把进程对应的GPU页表设置到硬件寄存器中，在细节上GPU的地址分配和映射和CPU稍有不同。 AMD GPU 的虚拟地址空间 (User Mode) 最新的AMD GPU 有48 bit VA 寻址能力，但是为了兼容32bit 系统4G地址空间，GPU 的虚拟地址被认为分成4个段： AMD GPU VA RANGES Start End Size 1. vamgr_32 0x0000_0000_0000_0000 0x0000_0000_FFFF_FFFF 4G 2. vamgr 0x0000_0000_1000_0000 0x0000_7FFF_FFFF_FFFF 128T - 4G 3. vamgr_high_32 0xFFFF_8000_0000_0000 0xFFFF_8000_FFFF_FFFF 4G 4. vamgr_high 0xFFFF_8001_0000_0000 0xFFFF_0000_0000_0000 128T - 4G AMD GPU 的虚拟地址分配 (User Mode)GPU的虚拟地址空间被分为4个区间，进程可以按照自身需要选择使用哪段地址，GPU 用户态的虚拟地址由libdrm_amdgpu分配和释放，内核的态的虚拟地址由KMD驱动管理分配，内核的地址分配和用户态稍有不同，后面会有介绍。 分配的虚拟地址区间不可以跨越上面的不同区域。 KMD会保留一部分VA作为特殊用途，并通过ioctl命令report给libdrm_amdgpu。 用户态不需要关心VA地址关联的内存是显存(VRAM)还是系统内存(GTT)，内存的类型会通过GPU的页表(PTE)隐藏掉，因此GTT和VRAM的寻址会被GPU VM硬件单元统一编址，简化用户态编程。 libdrm amdgpu vamgr 初始化 123456789101112131415161718192021222324252627282930drm_public int amdgpu_device_initialize(int fd, uint32_t *major_version, uint32_t *minor_version, amdgpu_device_handle *device_handle){ .... start = dev-&gt;dev_info.virtual_address_offset; max = MIN2(dev-&gt;dev_info.virtual_address_max, 0x100000000ULL); amdgpu_vamgr_init(&amp;dev-&gt;vamgr_32, start, max, dev-&gt;dev_info.virtual_address_alignment); start = max; max = MAX2(dev-&gt;dev_info.virtual_address_max, 0x100000000ULL); amdgpu_vamgr_init(&amp;dev-&gt;vamgr, start, max, dev-&gt;dev_info.virtual_address_alignment); start = dev-&gt;dev_info.high_va_offset; max = MIN2(dev-&gt;dev_info.high_va_max, (start &amp; ~0xffffffffULL) + 0x100000000ULL); amdgpu_vamgr_init(&amp;dev-&gt;vamgr_high_32, start, max, dev-&gt;dev_info.virtual_address_alignment); start = max; max = MAX2(dev-&gt;dev_info.high_va_max, (start &amp; ~0xffffffffULL) + 0x100000000ULL); amdgpu_vamgr_init(&amp;dev-&gt;vamgr_high, start, max, dev-&gt;dev_info.virtual_address_alignment);} libdrm amdgpu va 分配 amdgpu_vamgr_find_va： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960static drm_private uint64_tamdgpu_vamgr_find_va(struct amdgpu_bo_va_mgr *mgr, uint64_t size, uint64_t alignment, uint64_t base_required){ struct amdgpu_bo_va_hole *hole, *n; uint64_t offset = 0, waste = 0; alignment = MAX2(alignment, mgr-&gt;va_alignment); size = ALIGN(size, mgr-&gt;va_alignment); if (base_required % alignment) return AMDGPU_INVALID_VA_ADDRESS; pthread_mutex_lock(&amp;mgr-&gt;bo_va_mutex); LIST_FOR_EACH_ENTRY_SAFE_REV(hole, n, &amp;mgr-&gt;va_holes, list) { if (base_required) { if (hole-&gt;offset &gt; base_required || (hole-&gt;offset + hole-&gt;size) &lt; (base_required + size)) continue; waste = base_required - hole-&gt;offset; offset = base_required; } else { offset = hole-&gt;offset; waste = offset % alignment; waste = waste ? alignment - waste : 0; offset += waste; if (offset &gt;= (hole-&gt;offset + hole-&gt;size)) { continue; } } if (!waste &amp;&amp; hole-&gt;size == size) { offset = hole-&gt;offset; list_del(&amp;hole-&gt;list); free(hole); pthread_mutex_unlock(&amp;mgr-&gt;bo_va_mutex); return offset; } if ((hole-&gt;size - waste) &gt; size) { if (waste) { n = calloc(1, sizeof(struct amdgpu_bo_va_hole)); n-&gt;size = waste; n-&gt;offset = hole-&gt;offset; list_add(&amp;n-&gt;list, &amp;hole-&gt;list); } hole-&gt;size -= (size + waste); hole-&gt;offset += size + waste; pthread_mutex_unlock(&amp;mgr-&gt;bo_va_mutex); return offset; } if ((hole-&gt;size - waste) == size) { hole-&gt;size = waste; pthread_mutex_unlock(&amp;mgr-&gt;bo_va_mutex); return offset; } } pthread_mutex_unlock(&amp;mgr-&gt;bo_va_mutex); return AMDGPU_INVALID_VA_ADDRESS;} 个人感觉这个代码是可以用 inteval-tree 进行优化一下可以提高下分配效率，效率可以从O(n)减少为O(log(n)) amdgpu_vamgr_find_va是libdrm里分配VA的函数，该函数使用一个链表来管理VA的分配。 amdgpu_bo_va_mgr *mgr ： 对应上面的4个区间之一 uint64_t size: VA分配大小 uint64_t alignment：地址对齐要求， 一般4k对齐可以提高效率 base_required：最低地址要求 AMD GPU 的虚拟地址空间 (Kernel Mode) 在内核态不需要考虑4G空间问题，将整个256T虚拟地址空间划分为2个128T的虚拟地址空间，因为整个48bit的虚拟地址空间很大，内核将这部分虚拟地址空间又划分了几个不同的区域: Type Size Note VRAM 8G 映射 显存 到 KMD 虚拟地址空间 GART 512 M 映射 System Memory 到 KMD的虚拟地址空间 AGP ~~ 映射 System Memory 到 KMD的虚拟地址空间 AGP是一个古老显卡的接口，现在已经基本不用，驱动里借用AGP完成一些特殊的功能。 KMD的虚拟地址和UMD稍有不同，通常来讲UMD需要通过GPU的页表来访问显存和GTT内存，GPU的页表创建和映射造成了很多不便，KMD将显存(VRAM)完整的映射到了GPU一个连续的虚拟地址空间，这个空间可以看做是显存的一个映射，这样做的好处是GPU显存的物理地址和KMD的虚拟地址只存在一个偏移，驱动在访问显存的时只需要加上一个偏移就可以访问显存，映射关系如下（KMD）： GPU VA = GPU PA + OFFSET 这个映射关系和Linux内核态的VA到PA的映射很相似。__pa(addr) 除了访问GPU显存，KMD还需要访问System Memory， 这部分和UMD一样，通过GPU的页表来完成到物理地址的映射。 因为KMD不需要使用大量的系统内存，所以VA的地址空间只使用了512M，这样仅仅可以通过一级页表就可以完成内核的地址映射。 VA宽度会决定页表的级数，同时也会影响页表的翻译效率，因此KMD将VA宽度限制为512M。 以上的地址信息可以在Kernel Log里查看： 123[ 0.867957] amdgpu 0000:03:00.0: amdgpu: VRAM: 8176M 0x000000F400000000 - 0x000000F5FEFFFFFF (8176M used)[ 0.867957] amdgpu 0000:03:00.0: amdgpu: GART: 512M 0x0000000000000000 - 0x000000001FFFFFFF[ 0.867958] amdgpu 0000:03:00.0: amdgpu: AGP: 267419648M 0x000000F800000000 - 0x0000FFFFFFFFFFFF AMD GPU 的虚拟地址分配 (Kernel Mode)GTT 虚拟地址分配Linux DRM Framework 并没有提供一个完整的地址分配函数，这部分由KMD完成： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * amdgpu_gtt_mgr_alloc - allocate new ranges * * @man: TTM memory type manager * @tbo: TTM BO we need this range for * @place: placement flags and restrictions * @mem: the resulting mem object * * Allocate the address space for a node. */static int amdgpu_gtt_mgr_alloc(struct ttm_mem_type_manager *man, struct ttm_buffer_object *tbo, const struct ttm_place *place, struct ttm_mem_reg *mem){ struct amdgpu_device *adev = amdgpu_ttm_adev(man-&gt;bdev); struct amdgpu_gtt_mgr *mgr = man-&gt;priv; struct amdgpu_gtt_node *node = mem-&gt;mm_node; enum drm_mm_insert_mode mode; unsigned long fpfn, lpfn; int r; if (amdgpu_gtt_mgr_has_gart_addr(mem)) return 0; if (place) fpfn = place-&gt;fpfn; else fpfn = 0; if (place &amp;&amp; place-&gt;lpfn) lpfn = place-&gt;lpfn; else lpfn = adev-&gt;gart.num_cpu_pages; mode = DRM_MM_INSERT_BEST; if (place &amp;&amp; place-&gt;flags &amp; TTM_PL_FLAG_TOPDOWN) mode = DRM_MM_INSERT_HIGH; spin_lock(&amp;mgr-&gt;lock); r = drm_mm_insert_node_in_range(&amp;mgr-&gt;mm, &amp;node-&gt;node, mem-&gt;num_pages, mem-&gt;page_alignment, 0, fpfn, lpfn, mode); spin_unlock(&amp;mgr-&gt;lock); if (!r) mem-&gt;start = node-&gt;node.start; return r;} KMD驱动借助DRM提供的helper函数drm_mm_insert_node_in_range 来完成地址分配，mem-&gt;start就是返回的分配结果。 fpfn: First Page Frame Number lpfn: Last Page Frame Number mode = DRM_MM_INSERT_BEST: 地址分配策略，使用空闲区间中size最小的那个 drm_mm_xxxx 使用rbtree在内核态实现了一组地址分配函数，通过这些函数DRM driver可以用它来管理不同类型的资源： VA Address VRAM Address DRM BO VMA OFFSET Manager VRAM 虚拟地址分配KMD访问VRAM并不经过页表，因此不必为VRAM分配虚拟地址空间，仅仅将VRAM的地址加上一个偏移就可以访问。 GPU VA = GPU PA + OFFSET AMD GPU 的物理地址空间APU 的物理地址空间APU没有独立的显存，APU将系统的一段连续内存映作为自己的显存，这部分内存被System BIOSreserve后并上报给OS，因此从OS的视角来看，系统确实了块内存，通俗来讲OS并不参与管理这部分内存。另外这部分显存通常不会掉电丢失数据，这一点和dGPU不同。 dGPU的物理地址空间dGPU有独立的显存，这部分显存由KMD驱动管理和分配，本人所使用的RX5500显卡有8G显存，由于硬件和平台的一些限制，系统并不能访问全部的显存。 在关闭 Large Bar功能后，通常只有256MB的空间被映射到PCIE BAR空间，也就意味着CPU只能直接访问256M的显存，访问其他部分显存需要借助其他硬件来实现。 可以通过下面的命令来查看BAR0的地址 1234567891011121314$ lspci -d 1002: -vvvnnn2f:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Navi 14 [Radeon RX 5500/5500M / Pro 5500M] [1002:7340] (rev c5) (prog-if 00 [VGA controller]) Subsystem: Advanced Micro Devices, Inc. [AMD/ATI] Navi 14 [Radeon RX 5500/5500M / Pro 5500M] [1002:0b0c] Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+ Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx- Latency: 0, Cache Line Size: 64 bytes Interrupt: pin A routed to IRQ 83 Region 0: Memory at d0000000 (64-bit, prefetchable) [size=256M] Region 2: Memory at e0000000 (64-bit, prefetchable) [size=2M] Region 4: I/O ports at f000 [size=256] Region 5: Memory at fce00000 (32-bit, non-prefetchable) [size=512K] Expansion ROM at fce80000 [disabled] [size=128K] Capabilities: &lt;access denied&gt; Kernel driver in use: amdgpu 下面的命令可以帮你辅助验证这个BAR0就是你用的显存的最开始部分： 12345678910111213$ sudo hexdump -n 128 /sys/kernel/debug/dri/0/amdgpu_vram0000000 0028 0000 0038 0000 0001 0000 0000 00000000010 0001 0000 0000 0000 0001 0000 0028 00000000020 0010 0000 0000 0000 0007 0000 0000 00000000030 0780 0000 0440 0000 0000 0000 0000 00000000040 0000 0000 0000 0000 0000 0000 0000 0000$ sudo memtool md -b 0xd0000000+128d0000000: 28 00 00 00 38 00 00 00 01 00 00 00 00 00 00 00 (...8...........d0000010: 01 00 00 00 00 00 00 00 01 00 00 00 28 00 00 00 ............(...d0000020: 10 00 00 00 00 00 00 00 07 00 00 00 00 00 00 00 ................d0000030: 80 07 00 00 40 04 00 00 00 00 00 00 00 00 00 00 ....@...........d0000040: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 细心的你还会在kernel log里观察到 efifb用的地址就是 BAR0的地址 123456[ 0.374534] efifb: probing for efifb[ 0.374552] efifb: showing boot graphics[ 0.375508] efifb: framebuffer at 0xd0000000, using 8100k, total 8100k[ 0.375509] efifb: mode is 1920x1080x32, linelength=7680, pages=1[ 0.375510] efifb: scrolling: redraw[ 0.375510] efifb: Truecolor: size=8:8:8:8, shift=24:16:8:0 你可以直接操作 0xd0000000 这个地址来完成屏幕的绘画，需要注意的是，这个操作只有在load amdgpu driver之前才可以，加载驱动后，fb driver会从efifb切换到amdgpu_fb，对应着2套不同的驱动。当启动Xserver之后，这个amdgpu_fb驱动会被KMS驱动替换掉。 AMD GPU 的物理地址分配AMDGPU GPU的物理地址分配也是通过DRM Helperdrm_mm_insert_node_in_range实现的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116/** * amdgpu_vram_mgr_new - allocate new ranges * * @man: TTM memory type manager * @tbo: TTM BO we need this range for * @place: placement flags and restrictions * @mem: the resulting mem object * * Allocate VRAM for the given BO. */static int amdgpu_vram_mgr_new(struct ttm_mem_type_manager *man, struct ttm_buffer_object *tbo, const struct ttm_place *place, struct ttm_mem_reg *mem){ struct amdgpu_device *adev = amdgpu_ttm_adev(man-&gt;bdev); struct amdgpu_vram_mgr *mgr = man-&gt;priv; struct drm_mm *mm = &amp;mgr-&gt;mm; struct drm_mm_node *nodes; enum drm_mm_insert_mode mode; unsigned long lpfn, num_nodes, pages_per_node, pages_left; uint64_t vis_usage = 0, mem_bytes; unsigned i; int r; lpfn = place-&gt;lpfn; if (!lpfn) lpfn = man-&gt;size; /* bail out quickly if there's likely not enough VRAM for this BO */ mem_bytes = (u64)mem-&gt;num_pages &lt;&lt; PAGE_SHIFT; if (atomic64_add_return(mem_bytes, &amp;mgr-&gt;usage) &gt; adev-&gt;gmc.mc_vram_size) { atomic64_sub(mem_bytes, &amp;mgr-&gt;usage); mem-&gt;mm_node = NULL; return 0; } if (place-&gt;flags &amp; TTM_PL_FLAG_CONTIGUOUS) { pages_per_node = ~0ul; num_nodes = 1; } else {#ifdef CONFIG_TRANSPARENT_HUGEPAGE pages_per_node = HPAGE_PMD_NR;#else /* default to 2MB */ pages_per_node = (2UL &lt;&lt; (20UL - PAGE_SHIFT));#endif pages_per_node = max((uint32_t)pages_per_node, mem-&gt;page_alignment); num_nodes = DIV_ROUND_UP(mem-&gt;num_pages, pages_per_node); } nodes = kvmalloc_array((uint32_t)num_nodes, sizeof(*nodes), GFP_KERNEL | __GFP_ZERO); if (!nodes) { atomic64_sub(mem_bytes, &amp;mgr-&gt;usage); return -ENOMEM; } mode = DRM_MM_INSERT_BEST; if (place-&gt;flags &amp; TTM_PL_FLAG_TOPDOWN) mode = DRM_MM_INSERT_HIGH; mem-&gt;start = 0; pages_left = mem-&gt;num_pages; spin_lock(&amp;mgr-&gt;lock); for (i = 0; pages_left &gt;= pages_per_node; ++i) { unsigned long pages = rounddown_pow_of_two(pages_left); r = drm_mm_insert_node_in_range(mm, &amp;nodes[i], pages, pages_per_node, 0, place-&gt;fpfn, lpfn, mode); if (unlikely(r)) break; vis_usage += amdgpu_vram_mgr_vis_size(adev, &amp;nodes[i]); amdgpu_vram_mgr_virt_start(mem, &amp;nodes[i]); pages_left -= pages; } for (; pages_left; ++i) { unsigned long pages = min(pages_left, pages_per_node); uint32_t alignment = mem-&gt;page_alignment; if (pages == pages_per_node) alignment = pages_per_node; r = drm_mm_insert_node_in_range(mm, &amp;nodes[i], pages, alignment, 0, place-&gt;fpfn, lpfn, mode); if (unlikely(r)) goto error; vis_usage += amdgpu_vram_mgr_vis_size(adev, &amp;nodes[i]); amdgpu_vram_mgr_virt_start(mem, &amp;nodes[i]); pages_left -= pages; } spin_unlock(&amp;mgr-&gt;lock); atomic64_add(vis_usage, &amp;mgr-&gt;vis_usage); mem-&gt;mm_node = nodes; return 0;error: while (i--) drm_mm_remove_node(&amp;nodes[i]); spin_unlock(&amp;mgr-&gt;lock); atomic64_sub(mem-&gt;num_pages &lt;&lt; PAGE_SHIFT, &amp;mgr-&gt;usage); kvfree(nodes); return r == -ENOSPC ? 0 : r;} 虽然都是通过drm_mm_insert_node_in_range()的DRM helper实现的地址分配，但是分配细节上还是有一些不同的。gtt mgr分配的是虚拟地址，是要求地址连续的，因此驱动只需要准备一个node就可以接收分配结果，但是当分配VRAM的时候则没有这种要求，drm_mm可以返回一组node，这些node的总容量满足分配要求就可以，但是有时候内核也需要分配一段连续的显存给GPU使用，这个时候就只需要分配一个node就可以（参考 TTM_PL_FLAG_CONTIGUOUS）。当需要分配的size大于2M的时候，KMD会将其拆开多个2M的node，因为2M的node恰好满足huge page size，且GPU页表对2M的size有特殊优化。 总结相信你读到这里，已经对整个GPU内存管理有一个大概的了解，后面会更加深入的介绍AMD GPU内存管理的技术细节。 GPU的页表管理 CPU/GTT/VRAM状态的流转 GPU Page Fault UMD Access VRAM GPU VRAM Evict GPU P2P (Peer To Peer) IOMMU下的AMDGPU …..","link":"/2020/07/05/amdgpu/AMD_GPU_Virtual_Memory/"}],"tags":[{"name":"gpu","slug":"gpu","link":"/tags/gpu/"},{"name":"scheduler","slug":"scheduler","link":"/tags/scheduler/"},{"name":"kernel","slug":"kernel","link":"/tags/kernel/"},{"name":"amdgpu","slug":"amdgpu","link":"/tags/amdgpu/"},{"name":"iommu","slug":"iommu","link":"/tags/iommu/"},{"name":"amd64","slug":"amd64","link":"/tags/amd64/"},{"name":"virtualization","slug":"virtualization","link":"/tags/virtualization/"},{"name":"tool","slug":"tool","link":"/tags/tool/"}],"categories":[{"name":"gpu","slug":"gpu","link":"/categories/gpu/"},{"name":"amdgpu","slug":"amdgpu","link":"/categories/amdgpu/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"scheduler","slug":"gpu/scheduler","link":"/categories/gpu/scheduler/"},{"name":"iommu","slug":"iommu","link":"/categories/iommu/"},{"name":"tool","slug":"tool","link":"/categories/tool/"}]}